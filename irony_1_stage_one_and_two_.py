# -*- coding: utf-8 -*-
"""irony 1 stage one and two .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CgiQ91YFsmRkebU6rELRMLf1WXjVmQEQ
"""

from google.colab import files
# Upload files
uploaded = files.upload()



import pandas as pd

# Define a function to read and parse the text file
def load_dataset(file_path):
    # Read the text file line by line
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # Split each line into tweet and label
    data = [line.strip().rsplit('\t', maxsplit=1) for line in lines]
    tweets = [item[0] for item in data]
    labels = [int(item[1]) for item in data]

    # Return as a Pandas DataFrame
    return pd.DataFrame({'tweet': tweets, 'label': labels})

# Load train, dev, and test datasets
train_df = load_dataset('train.txt')
dev_df = load_dataset('dev.txt')
test_df = load_dataset('test.txt')

# Check the first few rows
print(train_df.head())

import re

# Function to remove links from a tweet
def remove_links(text):
    # Remove URLs using a regex
    return re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

# Apply the function to the tweet column
train_df['tweet'] = train_df['tweet'].apply(remove_links)
dev_df['tweet'] = dev_df['tweet'].apply(remove_links)
test_df['tweet'] = test_df['tweet'].apply(remove_links)

# Check the first few rows to verify
print(train_df.head())

!pip install nltk
!pip install textdistance

import re

# Normalize repeated characters
def normalize_repeated_chars(text):
    # Replace sequences of a single character (3+ times) with just one character
    return re.sub(r'(.)\1{2,}', r'\1', text)
# Example usage
text = "I loooove this soooo much!!! YAYYAYYAY"
normalized_text = normalize_repeated_chars(text)
print("Normalized Text:", normalized_text)

def split_repeated_words(text):
    # Match consecutive patterns of repeated words
    return re.sub(r'(\b\w+)\1{1,}',
                  lambda m: ' '.join([m.group(1)] * (len(m.group(0)) // len(m.group(1)))),
                  text)

# Example usage
text = "YAYYAYYAY"
split_text = split_repeated_words(text)
print("Split Text:", split_text)

!pip install wordsegment
from wordsegment import load, segment

# Load the word frequency dataset
load()
import re

def split_hashtags(text):
    # Find all hashtags in the text
    hashtags = re.findall(r'#\w+', text)

    for hashtag in hashtags:
        # Remove the "#" and segment the hashtag into words
        words = segment(hashtag[1:])  # Remove the "#" from the hashtag
        # Join the segmented words with spaces
        split_hashtag = ' '.join(words)
        # Replace the original hashtag in the text with the segmented version
        text = text.replace(hashtag, split_hashtag)

    return text
# Apply the split_hashtags function to all tweets
train_df['tweet'] = train_df['tweet'].apply(split_hashtags)
dev_df['tweet'] = dev_df['tweet'].apply(split_hashtags)
test_df['tweet'] = test_df['tweet'].apply(split_hashtags)

# Check results
print(train_df.head())

!pip install emoji
import emoji

# Function to replace emojis with their official names
def replace_emojis(text):
    return emoji.demojize(text, language='en')  # Replace emojis with names
# Apply emoji replacement to all datasets
train_df['tweet'] = train_df['tweet'].apply(replace_emojis)
dev_df['tweet'] = dev_df['tweet'].apply(replace_emojis)
test_df['tweet'] = test_df['tweet'].apply(replace_emojis)

# Save the preprocessed datasets to CSV files
train_df.to_csv('train_with_emojis.csv', index=False, encoding='utf-8')
dev_df.to_csv('dev_with_emojis.csv', index=False, encoding='utf-8')
test_df.to_csv('test_with_emojis.csv', index=False, encoding='utf-8')

print("CSV files saved successfully!")
from google.colab import files

# Download the saved CSV files
files.download('train_with_emojis.csv')
files.download('dev_with_emojis.csv')
files.download('test_with_emojis.csv')



import pandas as pd

# Load your uploaded CSV files
train_df = pd.read_csv('train_with_emojis.csv')
dev_df = pd.read_csv('dev_with_emojis.csv')

# Function to check preprocessing
def check_preprocessing(df, dataset_name):
    print(f"--- {dataset_name} Dataset ---")

    # Display basic info
    print("\nBasic Info:")
    print(df.info())

    # Display first few rows
    print("\nFirst 5 rows:")
    print(df.head())

    # Check for missing values
    print("\nMissing values:")
    print(df.isnull().sum())

    # Check if emojis are replaced with their names
    print("\nSample tweets with potential emoji replacements:")
    print(df['tweet'].iloc[:5])

    # Check if hashtags are split into words
    print("\nTweets containing hashtags:")
    print(df[df['tweet'].str.contains('#', na=False)].head())

    # Check if URLs are removed
    print("\nTweets containing potential URLs:")
    print(df[df['tweet'].str.contains('http|www', na=False)].head())

# Run checks
check_preprocessing(train_df, "Train")
check_preprocessing(dev_df, "Development")

import pandas as pd
import emoji
import re
from wordsegment import load, segment

# Step 1: Load the Data
train_df = pd.read_csv('train_with_emojis.csv')
dev_df = pd.read_csv('dev_with_emojis.csv')
test_df = pd.read_csv('test_with_emojis.csv')

# Step 2: Emoji Replacement
def replace_emojis(text):
    """Replace emojis with their official names."""
    return emoji.demojize(text, language='en')

train_df['tweet'] = train_df['tweet'].apply(replace_emojis)
dev_df['tweet'] = dev_df['tweet'].apply(replace_emojis)
test_df['tweet'] = test_df['tweet'].apply(replace_emojis)

# Step 3: Hashtag Splitting
load()  # Load word segmentation data

def split_hashtags(text):
    """Split hashtags into separate words."""
    words = []
    for token in text.split():
        if token.startswith('#'):
            words.extend(segment(token[1:]))  # Remove '#' and segment
        else:
            words.append(token)
    return ' '.join(words)

train_df['tweet'] = train_df['tweet'].apply(split_hashtags)
dev_df['tweet'] = dev_df['tweet'].apply(split_hashtags)
test_df['tweet'] = test_df['tweet'].apply(split_hashtags)

# Step 4: URL Removal
def remove_links(text):
    """Remove URLs from text."""
    return re.sub(r'https?://\S+|www\.\S+', '', text, flags=re.MULTILINE)

train_df['tweet'] = train_df['tweet'].apply(remove_links)
dev_df['tweet'] = dev_df['tweet'].apply(remove_links)
test_df['tweet'] = test_df['tweet'].apply(remove_links)

# Step 5: Save the Preprocessed Data
train_df.to_csv('train_fully_preprocessed.csv', index=False, encoding='utf-8')
dev_df.to_csv('dev_fully_preprocessed.csv', index=False, encoding='utf-8')
test_df.to_csv('test_fully_preprocessed.csv', index=False, encoding='utf-8')

print("Preprocessed datasets saved.")

# Step 6: Verification (Optional)
print("Sample tweets from Train Dataset after preprocessing:")
print(train_df['tweet'].head())

print("\nSample tweets from Dev Dataset after preprocessing:")
print(dev_df['tweet'].head())

print("\nSample tweets from Test Dataset after preprocessing:")
print(test_df['tweet'].head())

print("\nCheck for remaining hashtags in Train Dataset:")
print(train_df[train_df['tweet'].str.contains('#', na=False)].head())

print("\nCheck for remaining URLs in Train Dataset:")
print(train_df[train_df['tweet'].str.contains('http|www', na=False)].head())

from google.colab import files

# Download the preprocessed CSV files
files.download('train_fully_preprocessed.csv')
files.download('dev_fully_preprocessed.csv')
files.download('test_fully_preprocessed.csv')

import pandas as pd

# Load the preprocessed train and dev datasets
train_df = pd.read_csv('train_fully_preprocessed.csv')
dev_df = pd.read_csv('dev_fully_preprocessed.csv')

# Merge the train and dev datasets
merged_train_df = pd.concat([train_df, dev_df], ignore_index=True)

# Save the merged training dataset to a new CSV file
merged_train_df.to_csv('merged_train_set.csv', index=False, encoding='utf-8')

print("Merged training set saved as 'merged_train_set.csv'.")

# Verify the merged dataset
print(f"Number of rows in the merged training set: {len(merged_train_df)}")
print("First 5 rows of the merged training set:")
print(merged_train_df.head())

# prompt: downloading the file

files.download('merged_train_set.csv')

# prompt: import my files from my local

from google.colab import files

# Upload files
uploaded = files.upload()

# Step 1: Import Required Libraries
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
import joblib

# Step 2: Load the Data
# Load the merged training set and test set
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the data into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 3: Create Bag-of-Words Representation
# Initialize the CountVectorizer with a limit on the number of features
vectorizer = CountVectorizer(max_features=1000)

# Fit the vectorizer on the training data and transform both train and test sets
X_train_bow = vectorizer.fit_transform(X_train)
X_test_bow = vectorizer.transform(X_test)

# Step 4: Train the Naive Bayes Classifier
# Initialize and train the Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train_bow, y_train)

# Step 5: Evaluate the Model
# Evaluate the model's accuracy
nb_accuracy = nb_model.score(X_test_bow, y_test)
print(f"Naive Bayes Accuracy: {nb_accuracy * 100:.2f}%")

# Generate a classification report
nb_predictions = nb_model.predict(X_test_bow)
print("\nNaive Bayes Classification Report:")
print(classification_report(y_test, nb_predictions))

# Step 6: Save the Model and Vectorizer
# Save the trained model and vectorizer for future use
joblib.dump(vectorizer, 'bow_vectorizer.pkl')
joblib.dump(nb_model, 'naive_bayes_model.pkl')

print("\nModel and vectorizer saved as 'naive_bayes_model.pkl' and 'bow_vectorizer.pkl'.")

# Step 1: Import Required Libraries
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# Step 2: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 3: Create TF-IDF Representation
# Initialize the TF-IDF vectorizer with options for n-grams and stopwords
vectorizer = TfidfVectorizer(
    max_features=1000,           # Use the top 1000 features
    ngram_range=(1, 2),          # Include unigrams and bigrams
    stop_words='english'         # Remove common English stopwords
)

# Transform the training and test data
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 4: Train the Naive Bayes Classifier
# Initialize and train the Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train_tfidf, y_train)

# Step 5: Evaluate the Model
# Predict on the test set
predictions = nb_model.predict(X_test_tfidf)

# Print accuracy and classification report
accuracy = nb_model.score(X_test_tfidf, y_test)
print(f"Naive Bayes Accuracy with TF-IDF: {accuracy * 100:.2f}%\n")

print("Naive Bayes Classification Report with TF-IDF:")
print(classification_report(y_test, predictions))

# Step 6: Save the Model and Vectorizer (Optional)
import joblib

# Save the vectorizer and model for future use
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
joblib.dump(nb_model, 'naive_bayes_tfidf_model.pkl')

print("\nModel and vectorizer saved as 'naive_bayes_tfidf_model.pkl' and 'tfidf_vectorizer.pkl'.")

# Step 1: Import Required Libraries
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import classification_report

# Step 2: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 3: Create TF-IDF Representation
# Initialize the TF-IDF vectorizer
vectorizer = TfidfVectorizer(
    max_features=1000,           # Use the top 1000 features
    ngram_range=(1, 2),          # Include unigrams and bigrams
    stop_words='english'         # Remove common English stopwords
)

# Transform the training and test data
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 4: Train Complement Naive Bayes Classifier
# Initialize and train the Complement Naive Bayes model
cnb_model = ComplementNB()
cnb_model.fit(X_train_tfidf, y_train)

# Step 5: Evaluate the Model
# Predict on the test set
predictions = cnb_model.predict(X_test_tfidf)

# Print accuracy and classification report
accuracy = cnb_model.score(X_test_tfidf, y_test)
print(f"Complement Naive Bayes Accuracy with TF-IDF: {accuracy * 100:.2f}%\n")

print("Complement Naive Bayes Classification Report with TF-IDF:")
print(classification_report(y_test, predictions))

# Step 6: Save the Model and Vectorizer (Optional)
import joblib

# Save the vectorizer and model for future use
joblib.dump(vectorizer, 'tfidf_vectorizer_cnb.pkl')
joblib.dump(cnb_model, 'complement_naive_bayes_model.pkl')

print("\nModel and vectorizer saved as 'complement_naive_bayes_model.pkl' and 'tfidf_vectorizer_cnb.pkl'.")

from imblearn.over_sampling import SMOTE
from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import classification_report

# Step 1: Perform SMOTE Oversampling
print("Before SMOTE:")
print(f"Class distribution: {y_train.value_counts()}\n")

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

print("After SMOTE:")
print(f"Class distribution: {pd.Series(y_train_resampled).value_counts()}\n")

# Step 2: Train Complement Naive Bayes on the Resampled Data
cnb_model = ComplementNB()
cnb_model.fit(X_train_resampled, y_train_resampled)

# Step 3: Evaluate the Model
predictions = cnb_model.predict(X_test_tfidf)

# Step 4: Print Accuracy and Classification Report
accuracy = cnb_model.score(X_test_tfidf, y_test)
print(f"Complement Naive Bayes Accuracy with TF-IDF and SMOTE: {accuracy * 100:.2f}%\n")

print("Classification Report with SMOTE:")
print(classification_report(y_test, predictions))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import classification_report

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Create Character N-Gram Representation
# Initialize the TF-IDF vectorizer for character n-grams
vectorizer = TfidfVectorizer(
    analyzer='char',           # Use characters instead of words
    ngram_range=(3, 5),        # Use trigrams to 5-grams
    max_features=3000          # Increase the number of features to capture more patterns
)

# Transform the training and test data
X_train_char = vectorizer.fit_transform(X_train)
X_test_char = vectorizer.transform(X_test)

# Step 3: Train Complement Naive Bayes Classifier
# Initialize and train the Complement Naive Bayes model
cnb_model = ComplementNB()
cnb_model.fit(X_train_char, y_train)

# Step 4: Evaluate the Model
# Predict on the test set
predictions = cnb_model.predict(X_test_char)

# Print accuracy and classification report
accuracy = cnb_model.score(X_test_char, y_test)
print(f"Complement Naive Bayes Accuracy with Character N-Grams: {accuracy * 100:.2f}%\n")

print("Classification Report with Character N-Grams:")
print(classification_report(y_test, predictions))

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import classification_report
import pandas as pd

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Create Word and Character N-Gram Representations
# Word n-grams (unigrams and bigrams)
word_vectorizer = TfidfVectorizer(
    analyzer='word',
    ngram_range=(1, 2),  # Use unigrams and bigrams
    max_features=2000,   # Limit to top 2000 features
    stop_words='english'
)

# Character n-grams (trigrams to 5-grams)
char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5),  # Use trigrams to 5-grams
    max_features=2000    # Limit to top 2000 features
)

# Fit-transform on training data and transform test data
X_train_word = word_vectorizer.fit_transform(X_train)
X_test_word = word_vectorizer.transform(X_test)

X_train_char = char_vectorizer.fit_transform(X_train)
X_test_char = char_vectorizer.transform(X_test)

# Combine word and character n-gram features
X_train_combined = hstack([X_train_word, X_train_char])
X_test_combined = hstack([X_test_word, X_test_char])

# Step 3: Train Complement Naive Bayes Classifier
cnb_model = ComplementNB()
cnb_model.fit(X_train_combined, y_train)

# Step 4: Evaluate the Model
predictions = cnb_model.predict(X_test_combined)

# Print accuracy and classification report
accuracy = cnb_model.score(X_test_combined, y_test)
print(f"Complement Naive Bayes Accuracy with Combined Word and Character N-Grams: {accuracy * 100:.2f}%\n")

print("Classification Report with Combined Word and Character N-Grams:")
print(classification_report(y_test, predictions))

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import classification_report
import joblib

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Create Word and Character N-Gram Representations
# Word n-grams (unigrams and bigrams)
word_vectorizer = TfidfVectorizer(
    analyzer='word',
    ngram_range=(1, 2),  # Use unigrams and bigrams
    max_features=2000,   # Limit to top 2000 features
    stop_words='english'
)

# Character n-grams (trigrams to 5-grams)
char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5),  # Use trigrams to 5-grams
    max_features=2000    # Limit to top 2000 features
)

# Transform the training and test data
X_train_word = word_vectorizer.fit_transform(X_train)
X_test_word = word_vectorizer.transform(X_test)

X_train_char = char_vectorizer.fit_transform(X_train)
X_test_char = char_vectorizer.transform(X_test)

# Combine word and character n-gram features
X_train_combined = hstack([X_train_word, X_train_char])
X_test_combined = hstack([X_test_word, X_test_char])

# Step 3: Train Complement Naive Bayes Classifier
cnb_model = ComplementNB()
cnb_model.fit(X_train_combined, y_train)

# Step 4: Evaluate the Model
predictions = cnb_model.predict(X_test_combined)

# Print accuracy and classification report
accuracy = cnb_model.score(X_test_combined, y_test)
print(f"Final Complement Naive Bayes Accuracy: {accuracy * 100:.2f}%\n")

print("Final Complement Naive Bayes Classification Report:")
print(classification_report(y_test, predictions))

# Step 5: Save the Final Model and Vectorizers
# Save the word and character vectorizers
joblib.dump(word_vectorizer, 'final_word_vectorizer.pkl')
joblib.dump(char_vectorizer, 'final_char_vectorizer.pkl')

# Save the trained CNB model
joblib.dump(cnb_model, 'final_complement_naive_bayes_model.pkl')

print("\nFinal model and vectorizers saved as:")
print("- 'final_word_vectorizer.pkl'")
print("- 'final_char_vectorizer.pkl'")
print("- 'final_complement_naive_bayes_model.pkl'")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import pandas as pd
import joblib

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Create TF-IDF Representation
# Initialize TF-IDF vectorizer
vectorizer = TfidfVectorizer(
    ngram_range=(1, 2),  # Use unigrams and bigrams
    max_features=3000,   # Limit to top 3000 features
    stop_words='english' # Remove common English stopwords
)

# Fit-transform the training data and transform the test data
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 3: Train the SVM Classifier
# Initialize and train a LinearSVC
svm_model = LinearSVC(C=1.0, random_state=42)  # Default regularization strength
svm_model.fit(X_train_tfidf, y_train)

# Step 4: Evaluate the Model
# Predict on the test set
predictions = svm_model.predict(X_test_tfidf)

# Print accuracy and classification report
accuracy = svm_model.score(X_test_tfidf, y_test)
print(f"SVM Accuracy with TF-IDF: {accuracy * 100:.2f}%\n")

print("SVM Classification Report:")
print(classification_report(y_test, predictions))

# Step 5: Save the Model and Vectorizer
# Save the vectorizer and trained model for reuse
joblib.dump(vectorizer, 'svm_tfidf_vectorizer.pkl')
joblib.dump(svm_model, 'svm_model.pkl')

print("\nModel and vectorizer saved as:")
print("- 'svm_tfidf_vectorizer.pkl'")
print("- 'svm_model.pkl'")

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
import pandas as pd
import joblib

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Create Word and Character N-Gram Representations
# Word n-grams (unigrams and bigrams)
word_vectorizer = TfidfVectorizer(
    analyzer='word',
    ngram_range=(1, 2),  # Use unigrams and bigrams
    max_features=3000,   # Limit to top 3000 features
    stop_words='english'
)

# Character n-grams (trigrams to 5-grams)
char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5),  # Use trigrams to 5-grams
    max_features=2000    # Limit to top 2000 features
)

# Fit-transform on training data and transform test data
X_train_word = word_vectorizer.fit_transform(X_train)
X_test_word = word_vectorizer.transform(X_test)

X_train_char = char_vectorizer.fit_transform(X_train)
X_test_char = char_vectorizer.transform(X_test)

# Combine word and character n-gram features
X_train_combined = hstack([X_train_word, X_train_char])
X_test_combined = hstack([X_test_word, X_test_char])

# Step 3: Tune SVM Hyperparameters using GridSearchCV
# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10],           # Regularization strength
    'loss': ['hinge', 'squared_hinge']  # Loss function for LinearSVC
}

# Initialize a GridSearchCV with LinearSVC
svm_model = LinearSVC(random_state=42)
grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=2)

# Fit the model with training data
grid_search.fit(X_train_combined, y_train)

# Get the best model from grid search
best_svm = grid_search.best_estimator_
print(f"Best Hyperparameters: {grid_search.best_params_}")

# Step 4: Evaluate the Best SVM Model
# Predict on the test set
predictions = best_svm.predict(X_test_combined)

# Print accuracy and classification report
accuracy = best_svm.score(X_test_combined, y_test)
print(f"\nBest SVM Accuracy: {accuracy * 100:.2f}%\n")

print("Best SVM Classification Report:")
print(classification_report(y_test, predictions))

# Step 5: Save the Final Model and Vectorizers
# Save the word and character vectorizers
joblib.dump(word_vectorizer, 'best_word_vectorizer.pkl')
joblib.dump(char_vectorizer, 'best_char_vectorizer.pkl')

# Save the trained SVM model
joblib.dump(best_svm, 'best_svm_model.pkl')

print("\nFinal model and vectorizers saved as:")
print("- 'best_word_vectorizer.pkl'")
print("- 'best_char_vectorizer.pkl'")
print("- 'best_svm_model.pkl'")

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import pandas as pd
import joblib

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Create Word and Character N-Gram Representations
# Word n-grams (unigrams and bigrams)
word_vectorizer = TfidfVectorizer(
    analyzer='word',
    ngram_range=(1, 2),  # Use unigrams and bigrams
    max_features=3000,   # Limit to top 3000 features
    stop_words='english'
)

# Character n-grams (trigrams to 5-grams)
char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5),  # Use trigrams to 5-grams
    max_features=2000    # Limit to top 2000 features
)

# Fit-transform on training data and transform test data
X_train_word = word_vectorizer.fit_transform(X_train)
X_test_word = word_vectorizer.transform(X_test)

X_train_char = char_vectorizer.fit_transform(X_train)
X_test_char = char_vectorizer.transform(X_test)

# Combine word and character n-gram features
X_train_combined = hstack([X_train_word, X_train_char])
X_test_combined = hstack([X_test_word, X_test_char])

# Step 3: Train the Final SVM Model
# Initialize the SVM model with the best hyperparameters from grid search
final_svm_model = LinearSVC(C=1, loss='hinge', random_state=42)
final_svm_model.fit(X_train_combined, y_train)

# Step 4: Evaluate the Model
# Predict on the test set
predictions = final_svm_model.predict(X_test_combined)

# Print accuracy and classification report
accuracy = final_svm_model.score(X_test_combined, y_test)
print(f"Final SVM Accuracy: {accuracy * 100:.2f}%\n")

print("Final SVM Classification Report:")
print(classification_report(y_test, predictions))

# Step 5: Save the Final Model and Vectorizers
# Save the word and character vectorizers
joblib.dump(word_vectorizer, 'final_word_vectorizer.pkl')
joblib.dump(char_vectorizer, 'final_char_vectorizer.pkl')

# Save the trained SVM model
joblib.dump(final_svm_model, 'final_svm_model.pkl')

print("\nFinal model and vectorizers saved as:")
print("- 'final_word_vectorizer.pkl'")
print("- 'final_char_vectorizer.pkl'")
print("- 'final_svm_model.pkl'")

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
import pandas as pd
import joblib

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Create Bag-of-Words Representation
# Initialize CountVectorizer
vectorizer = CountVectorizer(
    max_features=3000,       # Limit to top 3000 features
    ngram_range=(1, 2),      # Use unigrams and bigrams
    stop_words='english'     # Remove common English stopwords
)

# Transform the training and test data
X_train_bow = vectorizer.fit_transform(X_train)
X_test_bow = vectorizer.transform(X_test)

# Step 3: Train Logistic Regression Classifier
# Initialize and train the Logistic Regression model
logreg_model = LogisticRegression(
    C=1.0,                  # Regularization strength
    max_iter=1000,          # Maximum number of iterations
    random_state=42         # Seed for reproducibility
)
logreg_model.fit(X_train_bow, y_train)

# Step 4: Evaluate the Model
# Predict on the test set
predictions = logreg_model.predict(X_test_bow)

# Print accuracy and classification report
accuracy = logreg_model.score(X_test_bow, y_test)
print(f"Logistic Regression Accuracy with Bag-of-Words: {accuracy * 100:.2f}%\n")

print("Logistic Regression Classification Report:")
print(classification_report(y_test, predictions))

# Step 5: Save the Model and Vectorizer
# Save the vectorizer and trained model for reuse
joblib.dump(vectorizer, 'logreg_bow_vectorizer.pkl')
joblib.dump(logreg_model, 'logreg_model.pkl')

print("\nModel and vectorizer saved as:")
print("- 'logreg_bow_vectorizer.pkl'")
print("- 'logreg_model.pkl'")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from scipy.sparse import hstack
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import pandas as pd
import joblib

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Create Word and Character N-Gram Representations
# Word n-grams (unigrams and bigrams)
word_vectorizer = TfidfVectorizer(
    analyzer='word',
    ngram_range=(1, 2),  # Use unigrams and bigrams
    max_features=3000,   # Limit to top 3000 features
    stop_words='english'
)

# Character n-grams (trigrams to 5-grams)
char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5),  # Use trigrams to 5-grams
    max_features=2000    # Limit to top 2000 features
)

# Fit-transform on training data and transform test data
X_train_word = word_vectorizer.fit_transform(X_train)
X_test_word = word_vectorizer.transform(X_test)

X_train_char = char_vectorizer.fit_transform(X_train)
X_test_char = char_vectorizer.transform(X_test)

# Combine word and character n-gram features
X_train_combined = hstack([X_train_word, X_train_char])
X_test_combined = hstack([X_test_word, X_test_char])

# Step 3: Tune SVM with GridSearchCV
# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],  # Regularization strengths
    'loss': ['hinge', 'squared_hinge']  # Loss functions for LinearSVC
}

# Initialize SVM
svm_model = LinearSVC(random_state=42, max_iter=10000)

# Perform grid search with 5-fold cross-validation
grid_search = GridSearchCV(
    estimator=svm_model,
    param_grid=param_grid,
    scoring='f1_macro',  # Use F1-macro to balance class importance
    cv=5,
    verbose=3,
    n_jobs=-1  # Use all available processors
)

# Fit the grid search on the training data
grid_search.fit(X_train_combined, y_train)

# Get the best model from grid search
best_svm = grid_search.best_estimator_
print(f"Best Hyperparameters: {grid_search.best_params_}")

# Step 4: Evaluate the Tuned Model
# Predict on the test set
predictions = best_svm.predict(X_test_combined)

# Print accuracy and classification report
accuracy = best_svm.score(X_test_combined, y_test)
print(f"\nTuned SVM Accuracy: {accuracy * 100:.2f}%\n")

print("Tuned SVM Classification Report:")
print(classification_report(y_test, predictions))

# Step 5: Save the Final Tuned Model and Vectorizers
# Save the word and character vectorizers
joblib.dump(word_vectorizer, 'tuned_word_vectorizer.pkl')
joblib.dump(char_vectorizer, 'tuned_char_vectorizer.pkl')

# Save the tuned SVM model
joblib.dump(best_svm, 'tuned_svm_model.pkl')

print("\nTuned model and vectorizers saved as:")
print("- 'tuned_word_vectorizer.pkl'")
print("- 'tuned_char_vectorizer.pkl'")
print("- 'tuned_svm_model.pkl'")

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import spacy
import joblib

# Step 1: Load the Data
# Load the preprocessed train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['tweet']
y_train = train_df['label']

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Initialize spaCy NLP Model for POS-Tagging
nlp = spacy.load("en_core_web_sm")

# Step 3: Function to Extract POS-Tag Frequencies
def extract_pos_features(text):
    doc = nlp(text)
    pos_counts = {pos: 0 for pos in ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'CONJ', 'INTJ']}
    for token in doc:
        if token.pos_ in pos_counts:
            pos_counts[token.pos_] += 1
    total_tokens = len(doc)
    # Normalize frequencies by dividing by total token count
    return [count / total_tokens if total_tokens > 0 else 0 for count in pos_counts.values()]

# Apply POS extraction to training and test sets
pos_columns = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'CONJ', 'INTJ']
train_pos = pd.DataFrame(X_train.apply(extract_pos_features).tolist(), columns=pos_columns)
test_pos = pd.DataFrame(X_test.apply(extract_pos_features).tolist(), columns=pos_columns)

# Step 4: Create TF-IDF Features for Word and Character N-Grams
# Word n-grams (unigrams and bigrams)
word_vectorizer = TfidfVectorizer(
    analyzer='word',
    ngram_range=(1, 2),  # Use unigrams and bigrams
    max_features=3000,   # Limit to top 3000 features
    stop_words='english'
)

# Character n-grams (trigrams to 5-grams)
char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5),  # Use trigrams to 5-grams
    max_features=2000    # Limit to top 2000 features
)

# Fit-transform on training data and transform test data
X_train_word = word_vectorizer.fit_transform(X_train)
X_test_word = word_vectorizer.transform(X_test)

X_train_char = char_vectorizer.fit_transform(X_train)
X_test_char = char_vectorizer.transform(X_test)

# Step 5: Combine TF-IDF Features and POS Features
# Convert POS features to sparse format
from scipy.sparse import csr_matrix
X_train_pos = csr_matrix(train_pos.values)
X_test_pos = csr_matrix(test_pos.values)

# Combine word, character, and POS features
X_train_combined = hstack([X_train_word, X_train_char, X_train_pos])
X_test_combined = hstack([X_test_word, X_test_char, X_test_pos])

# Step 6: Train the SVM Model
svm_model = LinearSVC(C=1, loss='hinge', random_state=42, max_iter=10000)
svm_model.fit(X_train_combined, y_train)

# Step 7: Evaluate the Model
predictions = svm_model.predict(X_test_combined)

# Print accuracy and classification report
accuracy = svm_model.score(X_test_combined, y_test)
print(f"SVM with POS Features Accuracy: {accuracy * 100:.2f}%\n")

print("SVM with POS Features Classification Report:")
print(classification_report(y_test, predictions))

# Step 8: Save the Model and Vectorizers
# Save the vectorizers and POS-tag model
joblib.dump(word_vectorizer, 'pos_word_vectorizer.pkl')
joblib.dump(char_vectorizer, 'pos_char_vectorizer.pkl')
joblib.dump(svm_model, 'svm_with_pos_model.pkl')

# Save POS column names for reuse
joblib.dump(pos_columns, 'pos_columns.pkl')

print("\nModel and vectorizers saved as:")
print("- 'pos_word_vectorizer.pkl'")
print("- 'pos_char_vectorizer.pkl'")
print("- 'svm_with_pos_model.pkl'")
print("- 'pos_columns.pkl'")

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
import joblib

# Step 1: Load Preprocessed Data
# Load train and test datasets
train_df = pd.read_csv('merged_train_set.csv')
test_df = pd.read_csv('test_fully_preprocessed.csv')

# Split train data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(
    train_df['tweet'], train_df['label'], test_size=0.2, random_state=42
)

X_test = test_df['tweet']
y_test = test_df['label']

# Step 2: Tokenize and Pad Sequences
# Tokenizer for converting text to sequences
tokenizer = Tokenizer(num_words=10000)  # Use top 10,000 most frequent words
tokenizer.fit_on_texts(X_train)

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences to ensure uniform length
max_len = 100  # Maximum tweet length
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

# Save the tokenizer
joblib.dump(tokenizer, 'tokenizer.pkl')
print("Tokenizer saved.")

# Step 3: Load GloVe Pretrained Embeddings
embedding_dim = 100  # Dimension of GloVe embeddings
embedding_index = {}

# Load GloVe embeddings
glove_file = 'glove.6B.100d.txt'  # Download from https://nlp.stanford.edu/projects/glove/
with open(glove_file, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embedding_index[word] = coefs

print(f"Loaded {len(embedding_index)} word vectors from GloVe.")

# Create an embedding matrix
vocab_size = len(tokenizer.word_index) + 1  # Include 0 for padding
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, i in tokenizer.word_index.items():
    if i < vocab_size:
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

# Step 4: Build the LSTM Model
model = Sequential([
    Embedding(input_dim=vocab_size,
              output_dim=embedding_dim,
              weights=[embedding_matrix],  # Pretrained embeddings
              input_length=max_len,
              trainable=False),  # Static embeddings
    Bidirectional(LSTM(128, return_sequences=True)),  # Bidirectional LSTM
    Dropout(0.5),
    LSTM(64),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Compile the model
model.compile(
    loss='binary_crossentropy',
    optimizer=Adam(learning_rate=0.001),
    metrics=['accuracy']
)

model.summary()

# Step 5: Train the Model
batch_size = 64
epochs = 10

history = model.fit(
    X_train_pad, y_train,
    validation_data=(X_val_pad, y_val),
    batch_size=batch_size,
    epochs=epochs,
    verbose=1
)

# Step 6: Evaluate the Model on Test Data
loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=1)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Step 7: Generate a Classification Report
y_pred = (model.predict(X_test_pad) > 0.5).astype('int32')
from sklearn.metrics import classification_report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Step 8: Save the Model
model.save('optimized_lstm_glove_model.h5')
print("Model saved as 'optimized_lstm_glove_model.h5'.")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Step 1: Build the Simplified LSTM Model
model = Sequential([
    Embedding(input_dim=vocab_size,
              output_dim=embedding_dim,
              weights=[embedding_matrix],  # Pretrained embeddings
              input_length=max_len,
              trainable=False),  # Static embeddings
    LSTM(64, return_sequences=False),  # Single LSTM layer with fewer units
    Dropout(0.3),                      # Lower dropout rate to prevent excessive regularization
    Dense(32, activation='relu'),      # Reduced dense layer size
    Dense(1, activation='sigmoid')     # Binary classification
])

# Step 2: Compile the Model
model.compile(
    loss='binary_crossentropy',
    optimizer=Adam(learning_rate=0.0005),  # Lower learning rate for more stable training
    metrics=['accuracy']
)

model.summary()

# Step 3: Train the Model
batch_size = 64
epochs = 10

history = model.fit(
    X_train_pad, y_train,
    validation_data=(X_val_pad, y_val),
    batch_size=batch_size,
    epochs=epochs,
    verbose=1
)

# Step 4: Evaluate the Model on Test Data
loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=1)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Step 5: Generate Classification Report
y_pred = (model.predict(X_test_pad) > 0.5).astype('int32')
from sklearn.metrics import classification_report
print("Classification Report:\n", classification_report(y_test, y_pred))