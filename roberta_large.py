# -*- coding: utf-8 -*-
"""roberta-large.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HoWOpq1WRuIgrtM9dJ0WO52AneN5mvuH
"""

# Step 1: Install Required Libraries
!pip install transformers datasets accelerate fsspec==2024.10.0 huggingface-hub -q

# Step 2: Authenticate with Hugging Face
from huggingface_hub import login

# Login using your Hugging Face token
login("YOUR_HF_TOKEN")  # Replace with your Hugging Face API token

# Step 3: Import Necessary Libraries
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import pandas as pd

# Step 4: Check GPU Availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Step 5: Load Preprocessed Data
# Paths for your uploaded datasets
train_file = '/content/merged_train_set.csv'
test_file = '/content/test_fully_preprocessed.csv'

# Load preprocessed CSV files into pandas DataFrames
train_df = pd.read_csv(train_file)
test_df = pd.read_csv(test_file)

# Split train data into train/validation sets
from sklearn.model_selection import train_test_split
train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)

# Convert DataFrames into Hugging Face Datasets
def convert_to_dataset(df, text_column, label_column):
    return Dataset.from_pandas(df[[text_column, label_column]].rename(columns={text_column: "text", label_column: "label"}))

train_dataset = convert_to_dataset(train_data, text_column="tweet", label_column="label")
val_dataset = convert_to_dataset(val_data, text_column="tweet", label_column="label")
test_dataset = convert_to_dataset(test_df, text_column="tweet", label_column="label")

# Combine into a DatasetDict
datasets = DatasetDict({
    "train": train_dataset,
    "validation": val_dataset,
    "test": test_dataset,
})

# Step 6: Load Pretrained RoBERTa-Large Model and Tokenizer
model_name = "roberta-large"  # Larger variant for better performance
tokenizer = RobertaTokenizer.from_pretrained(model_name)
model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)

# Step 7: Tokenize the Dataset
def tokenize_function(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)

tokenized_datasets = datasets.map(tokenize_function, batched=True)

# Remove unnecessary columns and set format for PyTorch
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets.set_format("torch")

# Step 8: Compute Class Weights
labels = train_data["label"].values
class_weights = compute_class_weight("balanced", classes=np.unique(labels), y=labels)
class_weights = torch.tensor(class_weights).to(device)

# Step 9: Define Custom Trainer with Weighted Loss
from transformers import Trainer

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        Custom loss function to incorporate class weights.
        """
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        loss = loss_function(logits, labels)
        return (loss, outputs) if return_outputs else loss

# Define Loss Function with Class Weights
from torch.nn import CrossEntropyLoss
loss_function = CrossEntropyLoss(weight=class_weights)

# Step 10: Define Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-5,  # Optimized learning rate
    per_device_train_batch_size=4,  # Reduced for RoBERTa-large (more memory needed)
    per_device_eval_batch_size=8,
    num_train_epochs=5,  # Increased epochs for better learning
    weight_decay=0.01,  # Regularization
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True,  # Enable mixed precision for faster training
    report_to="none"
)

# Step 11: Define a Metric Function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    acc = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

# Step 12: Initialize the Custom Trainer
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# Step 13: Train the Model
trainer.train()

# Step 14: Evaluate the Model on the Test Set
print("Evaluating on Test Set...")
metrics = trainer.evaluate(tokenized_datasets["test"])
print(metrics)

# Step 15: Save the Fine-Tuned Model
model.save_pretrained("/content/drive/MyDrive/1b/fine_tuned_roberta_large")
tokenizer.save_pretrained("/content/drive/MyDrive/1b/fine_tuned_roberta_large")
print("Model saved at: /content/drive/MyDrive/1b/fine_tuned_roberta_large")