# -*- coding: utf-8 -*-
"""effect of not proccing data hashtag and emoji.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qZ3wnP84uMozTbjJvidFjhGcvcsK3FUg
"""



from google.colab import files
# Upload files
uploaded = files.upload()

# Required Libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import pandas as pd
import joblib

# Step 1: Mount Google Drive to Access the Data (Optional)
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Load the Data
# Provide the correct path for train and test datasets
train_df = pd.read_csv('/content/train_converted.csv')
test_df = pd.read_csv('/content/test_converted.csv')

# Split the datasets into features (X) and labels (y)
X_train = train_df['text']  # Feature: tweet text
y_train = train_df['label']  # Label: sentiment (0 or 1)

X_test = test_df['text']
y_test = test_df['label']

# Step 3: Word and Character N-Gram Representations
# Word n-grams (unigrams and bigrams)
word_vectorizer = TfidfVectorizer(
    analyzer='word',
    ngram_range=(1, 2),  # Use unigrams and bigrams
    max_features=3000,   # Limit to top 3000 features
    stop_words='english'
)

# Character n-grams (trigrams to 5-grams)
char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5),  # Use trigrams to 5-grams
    max_features=2000    # Limit to top 2000 features
)

# Fit-transform on training data and transform test data
X_train_word = word_vectorizer.fit_transform(X_train)
X_test_word = word_vectorizer.transform(X_test)

X_train_char = char_vectorizer.fit_transform(X_train)
X_test_char = char_vectorizer.transform(X_test)

# Combine word and character n-gram features
X_train_combined = hstack([X_train_word, X_train_char])
X_test_combined = hstack([X_test_word, X_test_char])

# Step 4: Train the SVM Model
final_svm_model = LinearSVC(C=1, loss='hinge', random_state=42)
final_svm_model.fit(X_train_combined, y_train)

# Step 5: Evaluate the Model
# Predict on the test set
predictions = final_svm_model.predict(X_test_combined)

# Print accuracy and classification report
accuracy = final_svm_model.score(X_test_combined, y_test)
print(f"Final SVM Accuracy: {accuracy * 100:.2f}%\n")

print("Final SVM Classification Report:")
print(classification_report(y_test, predictions))

# Step 6: Save the Model and Vectorizers
# Save the vectorizers
joblib.dump(word_vectorizer, '/content/drive/MyDrive/final_word_vectorizer.pkl')
joblib.dump(char_vectorizer, '/content/drive/MyDrive/final_char_vectorizer.pkl')

# Save the trained SVM model
joblib.dump(final_svm_model, '/content/drive/MyDrive/final_svm_model.pkl')

print("\nFinal model and vectorizers saved as:")
print("- 'final_word_vectorizer.pkl'")
print("- 'final_char_vectorizer.pkl'")
print("- 'final_svm_model.pkl'")



# Required Libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Load the Data
# Correct paths to your datasets
train_df = pd.read_csv('/content/train_converted.csv')
test_df = pd.read_csv('/content/test_converted.csv')

# Split train data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(
    train_df['text'], train_df['label'], test_size=0.2, random_state=42)

X_test = test_df['text']
y_test = test_df['label']

# Step 3: Tokenize and Pad Sequences
max_words = 10000  # Limit vocabulary size
max_len = 100      # Max tweet length

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences to uniform length
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

# Step 4: Apply SMOTE for Oversampling
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_pad, y_train)

# Check class distribution after SMOTE
print("Class distribution after SMOTE:")
print(pd.Series(y_train_resampled).value_counts())

# Step 5: Load GloVe Embeddings
embedding_dim = 100
embedding_index = {}

glove_file = '/content/drive/MyDrive/1b/glove.6B.100d.txt'

# Load GloVe embeddings
with open(glove_file, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embedding_index[word] = coefs

# Create embedding matrix
vocab_size = len(tokenizer.word_index) + 1
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, i in tokenizer.word_index.items():
    if i < max_words:
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

# Step 6: Build the LSTM Model
model = Sequential([
    Embedding(input_dim=vocab_size,
              output_dim=embedding_dim,
              weights=[embedding_matrix],
              input_length=max_len,
              trainable=True),  # Allow fine-tuning of embeddings
    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),
    Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(
    loss='binary_crossentropy',
    optimizer=Adam(learning_rate=0.0001),
    metrics=['accuracy']
)

model.summary()

# Step 7: Train the Model
early_stopping = EarlyStopping(
    monitor='val_accuracy', patience=3, restore_best_weights=True
)

history = model.fit(
    X_train_resampled, y_train_resampled,
    validation_data=(X_val_pad, y_val),
    epochs=20,
    batch_size=64,
    callbacks=[early_stopping],
    verbose=1
)

# Step 8: Evaluate the Model
loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=1)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Generate predictions
y_pred = (model.predict(X_test_pad) > 0.5).astype('int32')

# Step 9: Classification Report
print("Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=['Non-Ironic', 'Ironic']))

# Step 10: Save the Model
model.save('/content/drive/MyDrive/final_lstm_glove_model_with_smote.keras')
print("LSTM model saved as 'final_lstm_glove_model_with_smote.keras'.")



# Step 1: Install Required Libraries
!pip install transformers datasets accelerate fsspec==2024.10.0 huggingface-hub -q

# Step 2: Import Libraries
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import pandas as pd

# Step 3: Check GPU Availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Step 4: Load Preprocessed Data
# Paths to your preprocessed data files
train_file = '/content/train_converted.csv'
test_file = '/content/test_converted.csv'

# Load data into Pandas DataFrames
train_df = pd.read_csv(train_file)
test_df = pd.read_csv(test_file)

# Split the training data into train and validation sets
train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)

# Convert DataFrames to Hugging Face Datasets
def convert_to_dataset(df, text_column, label_column):
    return Dataset.from_pandas(df[[text_column, label_column]].rename(columns={text_column: "text", label_column: "label"}))

train_dataset = convert_to_dataset(train_data, text_column="text", label_column="label")
val_dataset = convert_to_dataset(val_data, text_column="text", label_column="label")
test_dataset = convert_to_dataset(test_df, text_column="text", label_column="label")

# Combine datasets into a DatasetDict
datasets = DatasetDict({
    "train": train_dataset,
    "validation": val_dataset,
    "test": test_dataset,
})

# Step 5: Load Pretrained DeBERTa-v3-base Model and Tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)

# Step 6: Tokenize the Data
def tokenize_function(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)

tokenized_datasets = datasets.map(tokenize_function, batched=True)

# Remove unnecessary columns and set format for PyTorch
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets.set_format("torch")

# Step 7: Define Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,  # Optimized learning rate
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    num_train_epochs=5,  # Number of epochs
    weight_decay=0.01,  # Regularization
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True,  # Mixed precision for faster training
    report_to="none"
)

# Step 8: Define Metrics for Evaluation
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    acc = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

# Step 9: Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# Step 10: Train the Model
trainer.train()

# Step 11: Evaluate the Model on the Test Set
print("Evaluating on Test Set...")
metrics = trainer.evaluate(tokenized_datasets["test"])
print(metrics)

# Step 12: Save the Fine-Tuned Model
model.save_pretrained("/content/drive/MyDrive/fine_tuned_deberta_v3")
tokenizer.save_pretrained("/content/drive/MyDrive/fine_tuned_deberta_v3")
print("Model saved at: /content/drive/MyDrive/fine_tuned_deberta_v3")